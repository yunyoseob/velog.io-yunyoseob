ì˜¤ëŠ˜ í¬ìŠ¤íŒ…í•  2ë²ˆì§¸ **í‚¤ì›Œë“œë¡œ ê³µë¶€í•˜ëŠ” ë°ì´í„° ë¶„ì„**ì˜ í‚¤ì›Œë“œëŠ” Gradient Vanishingì…ë‹ˆë‹¤.

í¼ì…‰íŠ¸ë¡ ì´ë€ ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µìœ¼ë¡œë§Œ êµ¬ì„±ëœ ìµœì´ˆì˜ ì¸ê³µì‹ ê²½ë§ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

![](https://images.velog.io/images/yunyoseob/post/6b718ffa-2e99-46c2-9048-50fa1b6c6fa9/image.png)

- ì‚¬ì§„ì¶œì²˜ : [ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ : í¼ì…‰íŠ¸ë¡ ](https://wikidocs.net/24958)

> **ì‹¬ì¸µ ì‹ ê²½ë§ì€ ì€ë‹‰ì¸µì´ 2ê°œ ì´ìƒì¸ ì‹ ê²½ë§ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.**

Feed Forward Neural Network(ìˆœë°©í–¥ì‹ ê²½ë§)ì€ ì…ë ¥ë°ì´í„°ê°€ ì…ë ¥ì¸µ, ì€ë‹‰ì¸µ, ì¶œë ¥ì¸µ ìˆœì„œë¡œ ì „íŒŒë˜ì–´ íŒë³„ í•¨ìˆ˜ ê°’ìœ¼ë¡œ ë³€í™˜ë˜ëŠ” ì‹ ê²½ë§ì´ë©°, Back Propagation(ì—­ì „íŒŒ)ëŠ” ì—­ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê°±ì‹ ì„ í†µí•´ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”ì‹œí‚¤ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

> **Feed Forward Neural Network & Back Propagation**

![](https://images.velog.io/images/yunyoseob/post/e9e6fc26-6e9d-4d9c-8516-2466473d7901/image.png)

- ì‚¬ì§„ ì¶œì²˜ : [Research Gate](https://www.researchgate.net/figure/Feed-forward-back-propagation-mechanism-in-artificial-neural-network_fig1_336707258)

ì˜¤ëŠ˜ ë‹¤ë£° Gradient Vanishing(ê¸°ìš¸ê¸° ì†Œì‹¤) ë¬¸ì œëŠ” Back Propagationì—ì„œ ê³„ì‚° ê²°ê³¼ì™€ ì •ë‹µê³¼ì˜ ì˜¤ì°¨ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì •í•˜ëŠ”ë°, ì…ë ¥ì¸µìœ¼ë¡œ ê°ˆìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ ì‘ì•„ì ¸ ê°€ì¤‘ì¹˜ë“¤ì´ ì—…ë°ì´íŠ¸ ë˜ì§€ ì•Šì•„ ìµœì ì˜ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.

# Gradient Vanishing Problem : Sigmoid

Gradient Vanishg ë¬¸ì œëŠ” ì‹ ê²½ë§ì˜ í™œì„±í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ ê°’ì´ ê³„ì† ê³±í•´ì§€ë‹¤ ë³´ë©´ ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ ê²°ê³¼ê°’ì˜ ê¸°ìš¸ê¸°ê°€ 0ì´ ë˜ì–´ ë²„ë ¤ì„œ, Gradient Descent(ê²½ì‚¬í•˜ê°•ë²•)ì„ ì´ìš©í•  ìˆ˜ ì—†ê²Œ ë˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.

- **Gradient Descentì— ëŒ€í•œ ë‚´ìš©ì€ [ì´ì „ í¬ìŠ¤íŒ…](https://velog.io/@yunyoseob/Gradient-Descent-%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95)ì— ìˆìŠµë‹ˆë‹¤.**

> **ì˜ˆì‹œë¡œ, ë”¥ëŸ¬ë‹ì—ì„œ activation functionì„ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¡œ ì„¤ì •í–ˆë‹¤ê³  í•  ë•Œ, Gradient Vanishing Problemì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.**

![](https://images.velog.io/images/yunyoseob/post/1f809fcd-0121-461e-bc13-bc073fe234ef/image.png)

ë”¥ëŸ¬ë‹ì˜ Back Propagationì—ì„œ ìˆ˜ë§ì€ weight ë“¤ì„ ì—…ë°ì´íŠ¸ í•˜ê¸°ìœ„í•´, Chain rule(ë¯¸ë¶„ ì—°ì‡„ ë²•ì¹™)ì„ ì´ìš©í•  ë•Œ, sigmoid í•¨ìˆ˜ëŠ” 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ì„ ì¶œë ¥í•˜ë¯€ë¡œ, í•´ë‹¹ ê³¼ì •ì—ì„œ ë¯¸ë¶„ ê²°ê³¼ê°’ì´ ì†Œì‹¤ë˜ì–´ ë²„ë¦¬ëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. (ê²°ê³¼ ê°’ì´ 0ìœ¼ë¡œ ìˆ˜ë ´)

![](https://images.velog.io/images/yunyoseob/post/1af88f40-c995-4b10-82ff-28c03dd969eb/image.png)

ì¦‰, ë‹¤ìŒê³¼ ê°™ì€ ì‹ì´ ìˆì„ ë•Œ, 

![](https://images.velog.io/images/yunyoseob/post/09cd818a-795e-4893-b365-886e442c0ae3/image.png)

Gradient í•­ì´ ì‚¬ë¼ì ¸ ë²„ë¦¬ëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.


- sigmoid í•¨ìˆ˜ ë¯¸ë¶„ ê³¼ì • ì°¸ê³  ê¸€ : [ë‹µì„ ì°¾ì•„ê°€ëŠ” ê³¼ì • : ë”¥ëŸ¬ë‹ - í™œì„±í™” í•¨ìˆ˜ ì¢…ë¥˜ ë° ë¹„êµ](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=handuelly&logNo=221824080339)

# Gradient Vanishing Problem Solving

> **Activation Function : ReLU**

ë”¥ëŸ¬ë‹ì—ì„œëŠ” ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ReLU(Rectified Linear Unit, ê²½ì‚¬í•¨ìˆ˜)ë¥¼ ë„ì…í•˜ê¸°ë„ í•©ë‹ˆë‹¤. 

![](https://images.velog.io/images/yunyoseob/post/7afc54ba-9a63-438a-a945-c553f0ab3775/image.png)

ReLUì˜ ê²½ìš°, 0ë³´ë‹¤ ì‘ì€ ê°’ì€ 0ìœ¼ë¡œ ë°˜í™˜í•˜ê³ , 0ë³´ë‹¤ í° ê°’ì´ ë‚˜ì˜¬ ê²½ìš°, ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì—¬ Sigmoid í•¨ìˆ˜ë¥¼ Activation Functionìœ¼ë¡œ ì„¤ì •í–ˆì„ ë•Œ, ë°œìƒí•˜ëŠ” Gradient Vanishing Problemì„ ë³´ì™„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë”¥ëŸ¬ë‹ì—ì„œ ì´ì§„ë¶„ë¥˜ ë¬¸ì œë¥¼ ë‹¤ë£° ë•Œ, ë³´í†µ hidden layerì˜ activation functionì„ ReLUë¡œ ì„¤ì •í•˜ê³  ë§ˆì§€ë§‰ output layerì—ì„œë§Œ sigmoid í•¨ìˆ˜ë¥¼ ì“°ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.


> **Activation Function : Leaky ReLU**

ReLU í•¨ìˆ˜ì˜ ê²½ìš°, max(0,x)ì—ì„œ 0ìœ¼ë¡œ ê²°ê³¼ê°’ì´ ë„ì¶œëœë‹¤ë©´, ì´í›„ì— í•™ìŠµì´ ì´ë£¨ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. (ì´í›„ ì¶œë ¥ê°’ì´ ëª¨ë‘ 0ì´ ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.)

![](https://images.velog.io/images/yunyoseob/post/c5059f6b-5ea8-415f-b3e2-c5c458557585/image.png)

LeakyReLUëŠ” max(0,x)ì—ì„œ 0ëŒ€ì‹  ì•„ì£¼ ì‘ì€ ê°’ì„ xì— ê³±í•´ì£¼ì–´, 0ìœ¼ë¡œ ê²°ê³¼ê°’ì´ ë„ì¶œë  ë•Œ ë¬¸ì œì ì„ ë³´ì™„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤.

> **Batch Normalization(ë°°ì¹˜ ì •ê·œí™”)**

ë°°ì¹˜ ì •ê·œí™”ëŠ” ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì…ë ¥ì— ëŒ€í•´ í‰ê· ì„ 0ìœ¼ë¡œ ë§Œë“¤ê³ , ì •ê·œí™”ë¥¼ í•©ë‹ˆë‹¤. 

![](https://images.velog.io/images/yunyoseob/post/8429ec53-d603-4db8-b1e2-499afedad977/image.png)

- ë°°ì¹˜ ì •ê·œí™” ê³¼ì • : [ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ : ê¸°ìš¸ê¸° ì†Œì‹¤ê³¼ í­ì£¼](https://wikidocs.net/61375)

ë°°ì¹˜ ì •ê·œí™”ëŠ” ê° ì¸µì˜ ì…ë ¥ê°’ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ë‹¤ì‹œ ë§ì¶”ì–´ ì£¼ì–´, ì…ë ¥ê°’ì´ ì ë¦¬ëŠ” ê²ƒì„ ë§‰ì•„, ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì— í›¨ì”¬ ëœ ë¯¼ê°í•´ì§€ê³ , ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤.


> **Resnet(Residual Network)**

ì´ë¯¸ì§€ ë¶„ë¥˜ ë¬¸ì œì— ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” Convolutional Neural Networkì—ì„œ ë°œìƒí•˜ëŠ” Gradient Vanishing ë¬¸ì œë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•œ ëª¨ë¸ë¡œëŠ” Resnetì´ ìˆìŠµë‹ˆë‹¤. 

![](https://images.velog.io/images/yunyoseob/post/145a1e6f-c559-49a2-9698-dac04687139b/image.png)

- ì‚¬ì§„ ì¶œì²˜ : [Paper Review : ResNet](https://medium.com/humanscape-tech/paper-review-resnet-e768afd296bc)

Resnet ëª¨ë¸ì˜ ê²½ìš°, ì´ì „ Layerì˜ Feature Mapì„ ë‹¤ìŒ Layerì˜ Feature Mapì— ë”í•´ì¤ë‹ˆë‹¤. ì´ë¥¼ Skip Connectionì´ë¼ê³  í•©ë‹ˆë‹¤.  Resnetì˜ ê²½ìš°, ë”¥ëŸ¬ë‹ì—ì„œ Layerê°€ ê¹Šì–´ì§€ë©´ì„œ ë°œìƒí•˜ëŠ” Gradient Vanishing ë¬¸ì œë¥¼ Skip Connectionì„ í†µí•´ ìµœì†Œ gradientê°€ 1ì´ìƒ ê°’ì„ ê°–ê²Œ í•˜ì—¬, Gradient Vanishing ë¬¸ì œë¥¼ í•´ê²°í•œ ë°©ë²•ì…ë‹ˆë‹¤.


**ì´ìƒìœ¼ë¡œ í‚¤ì›Œë“œë¡œ ê³µë¶€í•˜ëŠ” ë°ì´í„° ë¶„ì„ì˜ ë‘ ë²ˆì§¸ í‚¤ì›Œë“œì¸ Gradient Vanishing í¬ìŠ¤íŠ¸ë¥¼ ë§ˆì¹˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ğŸ˜Š** 
