ì˜¤ëŠ˜ í¬ìŠ¤íŒ…í•  3ë²ˆì§¸ **í‚¤ì›Œë“œë¡œ ê³µë¶€í•˜ëŠ” ë°ì´í„° ë¶„ì„**ì˜ í‚¤ì›Œë“œëŠ” **Gaussian Distribution(ì •ê·œë¶„í¬)**ì…ë‹ˆë‹¤. 

## Gaussian Distribution

![](https://images.velog.io/images/yunyoseob/post/17e26465-c7d9-4707-a596-458ce7462b46/image.png)

 - ì‚¬ì§„ ì¶œì²˜ : [ì—£ì§€ìˆëŠ” ì¸ê³µì§€ëŠ¥ : ì •ê·œë¶„í¬](https://dlarhkd1211.tistory.com/93)

ì •ê·œë¶„í¬ëŠ” ì—°ì† í™•ë¥  ë¶„í¬ ì¤‘ í•˜ë‚˜ë¡œ, í‰ê· ì´ $\mu$ë¥¼ ë”°ë¥´ê³ , ë¶„ì‚°ì´ $\sigma^2$ì„ ë”°ë¥´ëŠ” ë¶„í¬ì´ë‹¤.

> **ë¹…ë°ì´í„°ì—ì„œ ì •ê·œë¶„í¬ê°€ ì¤‘ìš”í•œ ì´ìœ **

í‘œë³¸ì´ í¬ê¸°ê°€ ì»¤ì§ˆ ê²½ìš° ì¤‘ì‹¬ê·¹í•œ ì •ë¦¬(CLT : Central Limit Theorem)ì— ë”°ë¼ í‘œë³¸ì´ ì¶©ë¶„íˆ í° ê²½ìš°, ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ê²Œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 

ë°ì´í„° ë¶„ì„ì—ì„œ ì •ê·œë¶„í¬ë¥¼ í•˜ì§€ ì•ŠëŠ” ë³€ìˆ˜ë“¤ì˜ ê²½ìš°ì—ë„ Scaling, ë¡œê·¸ë³€í™˜ ë“±ì„ í†µí•´ ì •ê·œë¶„í¬ì— ê·¼ì‚¬í•˜ë„ë¡ ìœ ë„í•˜ì—¬ í•™ìŠµì„ ì‹œí‚¤ê¸°ë„ í•©ë‹ˆë‹¤.  

ë§ˆì§€ë§‰ìœ¼ë¡œ ì¸ê³µì§€ëŠ¥ì—ì„œ í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ì— í•™ìŠµì„ ì‹œí‚¬ ë•Œ, ëª¨ë¸ì˜ ê¸°ë³¸ ê°€ì •ì¤‘ì— ì •ê·œë¶„í¬ë¥¼ ê°€ì •ì‚¬í•­ìœ¼ë¡œ ì •í•˜ëŠ” ê²½ìš°ë„ ìˆìœ¼ë¯€ë¡œ, ì •ê·œë¶„í¬ì— ê·¼ì‚¬í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•´ì•¼ í•™ìŠµë¥ ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

> **ì •ê·œë¶„í¬ì˜ ì„±ì§ˆ**

1. ì •ê·œë¶„í¬ì—ì„œì˜ ê¸°ëŒ“ê°’, ìµœë¹ˆê°’, ì¤‘ì•™ê°’ ëª¨ë‘ $\mu$ì´ë‹¤.

2. í‰ê· ê³¼ í‘œì¤€í¸ì°¨ê°€ ì£¼ì–´ì ¸ ìˆì„ ë•Œ, ì—”íŠ¸ë¡œí”¼ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ë¶„í¬ì´ë‹¤.

3. ì •ê·œë¶„í¬ê³¡ì„ ì€ ì¢Œìš° ëŒ€ì¹­ì´ë©°, í•˜ë‚˜ì˜ ê¼­ì§€ë¥¼ ê°€ì§„ë‹¤.


> **ì •ê·œë¶„í¬ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜(pdf)**

$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-1}{2}(\frac{x-\mu}{\sigma})^2}, -\infty < x < \infty$

> **ì •ê·œë¶„í¬ Maximum Likelihood Estimation**

**1. ë¶„ì‚°ì„ ì•Œì§€ë§Œ í‰ê· ì„ ëª¨ë¥¼ ê²½ìš°,
( $\sigma^2 = \sigma_0^2$ë¡œ ì•Œë ¤ì ¸ ìˆì§€ë§Œ, $\mu$ëŠ” ì•Œë ¤ì ¸ ìˆì§€ ì•Šì€ ê²½ìš°)**

$L(\mu, \theta) = (2\pi\theta)^{\frac{-n}{2}}e^{\frac{- \sum_{i=1}^n (x_o - \mu)^2}{2\theta}}$

$\sum_{i=1}^n x_i = n\mu$

$\mu=\bar{x}$

**2. í‰ê· ì´ ì•Œë ¤ì ¸ ìˆê³ , ë¶„ì‚°ì„ ëª¨ë¥´ëŠ” ìƒíƒœ**

$\hat{\theta} = \hat{\sigma^2} = \frac{\sum_{i=1} (x_i - \mu_0)^2}{n}$

**3. í‰ê· ê³¼ ë¶„ì‚° ë‘˜ ë‹¤ ëª¨ë¥´ëŠ” ìƒíƒœ**

$\mu = \bar{x}$

$\hat{\theta} = \hat{\sigma^2} = \frac{\sum_{i=1} (x_i - \mu_0)^2}{n} = \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{n}$

## ë°ì´í„° ë¶„ì„ì—ì„œ Gaussian Distribution í™œìš© ì‚¬ë¡€

> **Anomaly Detection : ì´ìƒì¹˜ íƒì§€**

![](https://images.velog.io/images/yunyoseob/post/4ceba085-0481-4870-8278-0b31f4df0a96/image.png)

ì •ê·œë¶„í¬ì—ì„œ í‰ê· ì—ì„œ ì–‘ìª½ìœ¼ë¡œ ní‘œì¤€í¸ì°¨ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê°’ë“¤ì„ ì œê±°í•˜ëŠ” ë°©ë²•ì„ n-sigmaë¼ê³  í•œë‹¤.

ì˜ˆì‹œë¡œ, 3-sigmaëŠ” í‰ê· ì—ì„œ ì–‘ìª½ìœ¼ë¡œ 3í‘œì¤€í¸ì°¨ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê°’ì„ íƒì§€í•˜ëŠ” ì´ìƒì¹˜ íƒì§€ ë°©ë²•ì„ 3sigmaë²•ì´ë¼ê³  í•©ë‹ˆë‹¤.

> **Feature Scaling : Standard Scaler**

ì •ê·œë¶„í¬ ì¤‘ í‰ê· ì´ 0, ë¶„ì‚°ì´ 1ì¸ ì •ê·œë¶„í¬ë¥¼ í‘œì¤€ì •ê·œë¶„í¬(standard normal distribution)ì´ë¼ê³  í•©ë‹ˆë‹¤.

Feature Scalingì´ë€ Featureë“¤ì˜ í¬ê¸°ì™€ ë²”ìœ„ë¥¼ í‘œì¤€í™” í˜¹ì€ ì •ê·œí™” í•´ì£¼ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

**í‘œì¤€í™”**

$x=\frac{x-\mu}{\sigma}$

Standard Scalingì€ Featureë“¤ì˜ í¬ê¸°ì™€ ë²”ìœ„ë¥¼ í‘œì¤€í™”ë¥¼ í•´ì¤Œìœ¼ë¡œ ì¨, í‘œì¤€ì •ê·œë¶„í¬ë¥¼ ê°€ì§„ ê°’ìœ¼ë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.

> **Batch Normalization**

Batch Normalization(ë°°ì¹˜ ì •ê·œí™”)ëŠ” ì§€ë‚œ í¬ìŠ¤íŠ¸ [Gradient Vanishing : ê¸°ìš¸ê¸° ì†Œì‹¤](https://velog.io/@yunyoseob/Gradient-Vanishing-%EA%B8%B0%EC%9A%B8%EA%B8%B0-%EC%86%8C%EC%8B%A4)ì—ì„œë„ ë‹¤ë¤˜ë“¯, ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì…ë ¥ì— ëŒ€í•´ í‰ê· ì„ 0ìœ¼ë¡œ ë§Œë“¤ê³ , ì •ê·œí™”ë¥¼ í•©ë‹ˆë‹¤. 

ì´ ê³¼ì •ì—ì„œ, ê° ë¯¸ë‹ˆ ë°°ì¹˜ë“¤ì´ í‘œì¤€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ë„ë¡ ê°•ì œí•˜ì—¬, Local Optimum  ë¬¸ì œì— ë¹ ì§€ëŠ” ê°€ëŠ¥ì„±ì„ ì¤„ì´ê³ , Global Optimum ì§€ì ì„ ì°¾ë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.

> **Machine Learning Model Normality Assumption : ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì •ê·œë¶„í¬ ê°€ì •**

**ì„ í˜•íšŒê·€ë¶„ì„ ê°€ì • : ì •ê·œì„±**

ì”ì°¨í•­ì´ ì •ê·œë¶„í¬ì˜ í˜•íƒœë¥¼ ë„ì–´ì•¼ í•œë‹¤.

**ì„ í˜• íŒë³„ ë¶„ì„(LDA: Linear Discriminant Analysis)**

ê° í´ë˜ìŠ¤ ì§‘ë‹¨ì´ ì •ê·œë¶„í¬ì˜ í˜•íƒœì˜ í™•ë¥  ë¶„í¬ë¥¼ ê°€ì§„ë‹¤ê³  ê°€ì •í•œë‹¤.

**Gaussian Naive Bayes**

ì—°ì†ì ì¸ ê°’ì„ ì§€ë‹Œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ, ê° í´ë˜ìŠ¤ì˜ ì—°ì†ì ì¸ ê°’ë“¤ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•œë‹¤.

**ARIMA**
ì”ì°¨í•­ì´ ì •ê·œë¶„í¬ë¥¼ ê°€ì ¸ì•¼ í•œë‹¤.

**ì´ ì™¸ì—ë„ ëª¨ìˆ˜ê²€ì •ì€ ëª¨ì§‘ë‹¨ì´ ì •ê·œë¶„í¬ë¥¼ ì´ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤.**

**+Gaussianì´ë¼ëŠ” ë‹¨ì–´ê°€ ë“¤ì–´ê°€ëŠ” ëª¨ë¸ë¡œëŠ” Gaussian Process, SVM Gaussian Kernel ë“±ì´ ìˆìŠµë‹ˆë‹¤.**

## Gaussian Mixture Model

Guassian Distributionì— ëŒ€í•´ ì•Œì•„ë³´ê³ , ë°ì´í„° ë¶„ì„ì—ì„œ ì´ìƒì¹˜ íƒì§€, í”¼ì³ ìŠ¤ì¼€ì¼ë§, ì •ê·œì„± ê°€ì •ì´ ìˆëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ê³¼ Gaussianì´ë¼ëŠ” ì´ë¦„ì´ ë“¤ì–´ê°€ëŠ” ëª¨ë¸ë“¤ë„ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤.

ì˜¤ëŠ˜ í¬ìŠ¤íŒ…ì€ ë§ˆì§€ë§‰ìœ¼ë¡œ Gaussian Mixture Modelì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ë§ˆì¹˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. Gaussian Mixture Modelì„ ì„¤ëª…í•˜ê¸° ì•ì„œì„œ, Maximum Likelihood Estimation(MLE)ê³¼ EM ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì´í•´ê°€ ìˆì–´ì•¼ í•˜ë¯€ë¡œ, MLEì™€ EM ì•Œê³ ë¦¬ì¦˜ë¶€í„° ëŒ€í•´ ë¨¼ì € í¬ìŠ¤íŒ…í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. 

### Maximum Likelihood Estimation(MLE)

Maximum Likelihood Estimation(ìµœëŒ€ìš°ë„ë²•)ì€ ëª¨ìˆ˜ì ì¸ ë°ì´í„° ë°€ë„ ì¶”ì •ë°©ë²•ìœ¼ë¡œì¨ íŒŒë¼ë¯¸í„°$\theta = (\theta_1, ,\cdots, \theta_m)$ ë¡œ êµ¬ì„±ëœ ì–´ë–¤ í™•ë¥ ë°€ë„í•¨ìˆ˜ $P(x|\theta)$ì—ì„œ ê´€ì¸¡ëœ í‘œë³¸ ë°ì´í„° ì§‘í•©ì„ $x=(x_1, x_2, ,\cdots, x_n)$ì´ë¼ í•  ë•Œ, ì´ í‘œë³¸ë“¤ì—ì„œ íŒŒë¼ë¯¸í„° $\theta = (\theta_1, \cdots , \theta_m)$ì„ ì¶”ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

ì—¬ê¸°ì„œ, LikelihoodëŠ” ì§€ê¸ˆ ì–»ì€ ë°ì´í„°ê°€ ì–´ë– í•œ ë¶„í¬ë¡œ ë‚˜ì™”ì„ ê°€ëŠ¥ë„ë¥¼ ì˜ë¯¸í•˜ë©°, ìˆ˜ì¹˜ì ìœ¼ë¡œ ì´ ê°€ëŠ¥ë„ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ ê° ë°ì´í„° ìƒ˜í”Œì—ì„œ í›„ë³´ ë¶„í¬ì— ëŒ€í•œ ë†’ì´(ì¦‰, likelihood ê¸°ì—¬ë„)ë¥¼ ê³„ì‚°í•˜ì—¬ ë‹¤ ê³±í•©ë‹ˆë‹¤.

> **likelihood function**

$P(x|\theta)=\prod_{k=1}^n P(x_k|\theta)$

likelihood ê¸°ì—¬ë„ë¥¼ ë‹¤ ê³±í•´ì„œ ê³„ì‚°í•˜ëŠ” ê²ƒë³´ë‹¤ logì˜ ì„±ì§ˆì„ ì´ìš©í•˜ë©´, ì „ë¶€ ë”í•´ì„œ ê³„ì‚°í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê³„ì‚°ì˜ í¸ì˜ë¥¼ ìœ„í•´ ìì—°ë¡œê·¸ë¥¼ ì´ìš©í•´ log-likelihood functionì„ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.

> **log-likelihood**

$L(\theta|x) = \log P(x|\theta) = \sum_{i=1}^{n} \log P(x_{i} | \theta)$

ì´ì œ, ì°¾ê³ ì í•˜ëŠ” íŒŒë¼ë¯¸í„° $\theta$ì— ëŒ€í•˜ì—¬ í¸ë¯¸ë¶„í•˜ê³  ê·¸ ê°’ì´ 0ì´ ë˜ë„ë¡ í•˜ëŠ” $\theta$ë¥¼ ì°¾ì•„ MLEë¥¼ êµ¬í•©ë‹ˆë‹¤.

$\frac{\partial}{\partial \theta} L(\theta|x) = \frac{\partial}{\partial \theta} \log P(x|\theta) = \sum_{i=1}^{n}  \frac{\partial}{\partial \theta} \log P(x_{i}|\theta) = 0$

### EM algorithm

> **EM algorithmì€ Expectation-Maximizationì˜ ì•½ìë¡œ, log-likelihoodì˜ ê¸°ëŒ“ê°’ì„ ê³„ì‚°í•˜ëŠ” ê³¼ì •ê³¼ MLEë¥¼ ìˆ˜í–‰í•´ì„œ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

í¸ì˜ìƒ ëª¨ì§‘ë‹¨ì„ êµ¬ì„±í•˜ëŠ” ê° ì§‘ë‹¨ì˜ ë¶„í¬ëŠ” **ì •ê·œë¶„í¬**ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•œë‹¤. ê° ìë£Œê°€ ì–´ëŠ ì§‘ë‹¨(í´ë˜ìŠ¤)ìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ ê²ƒì¸ì§€ë¥¼ ì•ˆë‹¤ë©´ í•´ë‹¹ ëª¨ìˆ˜ì˜ ì¶”ì •ì´ ì–´ë µì§€ ì•Šì„ ê²ƒì´ë‹¤.

ê·¸ëŸ¬ë‚˜ ëª¨ì§‘ë‹¨ìœ¼ë¡œë¶€í„° ê° ë°ì´í„°ê°€ ì–´ëŠ ì§‘ë‹¨(í´ë˜ìŠ¤)ìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ ê²ƒì¸ì§€ë¥¼ ëª¨ë¥´ë¯€ë¡œ, ì´ ì •ë³´ë§Œì„ ìë£Œë¡œë¶€í„° ì¶”ì •í•  ìˆ˜ ìˆë‹¤ë©´ ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •ì˜ ë¬¸ì œëŠ” ì‰½ê²Œ í•´ê²°ë  ê²ƒì´ë‹¤. ë”°ë¼ì„œ ê° ìë£Œê°€ ì–´ëŠ ì§‘ë‹¨ì— ì†í•˜ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì§€ëŠ” ì ì¬ë³€ìˆ˜(latent variable)ë¥¼ ë„ì…í•œë‹¤.

> **E-ë‹¨ê³„**

ì ì¬ë³€ìˆ˜ë¥¼ $Z$ë¼ í•  ë•Œ, ëª¨ìˆ˜ì— ëŒ€í•œ ì´ˆê¸°ê°’ì´ ì£¼ì–´ì ¸ ìˆë‹¤ë©´ (ì¦‰, ì´ˆê¸° ë¶„í¬ë¥¼ ì•ˆë‹¤ë©´) ê° ìë£Œê°€ ì–´ëŠ ì§‘ë‹¨ìœ¼ë¡œë¶€í„° ë‚˜ì˜¬ í™•ë¥ ì´ ë†’ì€ì§€ì— ëŒ€í•´ ì¶”ì •ì´ ê°€ëŠ¥í•˜ë‹¤.


ë§Œì•½ ì§‘ë‹¨ì´ 2ê°œì´ê³ , ëª¨ë“  ìë£Œì— ëŒ€í•´ 1ì§‘ë‹¨ì— ì†í•  í™•ë¥ ì´ ê·¹ë‹¨ì ìœ¼ë¡œ 0 ë˜ëŠ” 1ë¡œë§Œ ì¶”ì •ë˜ì—ˆë‹¤ë©´, ì´ëŠ” ê° ìë£Œê°€ ì–´ëŠ ì§‘ë‹¨ì— ì†í•˜ëŠ”ì§€ë¥¼ ì•„ëŠ” ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ì´ë¯€ë¡œ ëª¨ìˆ˜ì— ëŒ€í•œ ì¶”ì •ì´ ì‰½ê²Œ ì´ë£¨ì–´ì§„ë‹¤. ì¦‰, ê° ìë£Œì— ëŒ€í•´ Zì˜ ì¡°ê±´ë¶€ë¶„í¬(ì–´ëŠ ì§‘ë‹¨ì— ì†í•  ì§€ì— ëŒ€í•œ)ë¡œë¶€í„° ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì„ êµ¬í•  ìˆ˜ ìˆë‹¤. (ì„ì˜ì˜ íŒŒë¼ë¯¸í„° ê°’ì„ ì •í•˜ê³  Z ê¸°ëŒ€ì¹˜ ê³„ì‚°)

> **M-ë‹¨ê³„**

ê´€ì¸¡ë³€ìˆ˜ $X$ì™€ ì ì¬ë³€ìˆ˜ $Z$ë¥¼ í¬í•¨í•˜ëŠ” $(X,Z)$ì— ëŒ€í•œ log-likelihood function(ì´ë¥¼ ë³´ì •ëœ(augmented) log-likelihood functionë¼ í•¨)ì—  ëŒ€ì‹  ìƒìˆ˜ê°’ì¸ ì˜ ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì„ ëŒ€ì…í•˜ë©´, log-likelihood functionë¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” ëª¨ìˆ˜ë¥¼ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆë‹¤.

Z ëŒ€ì‹  ìƒìˆ˜ê°’ì¸ Zì˜ ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì„ ëŒ€ì…í•˜ë©´, log-likelihood functionë¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” ëª¨ìˆ˜ë¥¼ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆë‹¤. ê°±ì‹ ëœ ëª¨ìˆ˜ ì¶”ì •ì¹˜ì— ëŒ€í•´ ìœ„ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤ë©´ ìˆ˜ë ´í•˜ëŠ” ê°’ì„ ì–»ê²Œ ë˜ê³ , ì´ëŠ” MLEë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. (Zì˜ ê¸°ëŒ€ì¹˜ë¥¼ ì´ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„° ì¶”ì • í›„, if likelihood == max -> íŒŒë¼ë¯¸í„°ì¶”ì •ê°’ ë„ì¶œ, else: -> ì¶”ì •ëœ íŒŒë¼ë¯¸í„°ë¥¼ í† ëŒ€ë¡œ ë‹¤ì‹œ Z ê¸°ëŒ€ì¹˜ ê³„ì‚°) => ì´ ê³¼ì •ì˜ íšŸìˆ˜ë¥¼ L(ë°˜ë³µ íšŸìˆ˜)ë¡œ í‘œí˜„

**ì°¸ê³ í•˜ê¸° ì¢‹ì€ ìë£Œ : [ê³µëŒì´ì˜ ìˆ˜í•™ì •ë¦¬ë…¸íŠ¸ : GMMê³¼ EM ì•Œê³ ë¦¬ì¦˜](https://angeloyeo.github.io/2021/02/08/GMM_and_EM.html)**

### Gaussian Mixture Model

Gaussian Mixture Modelì€ ë°ì´í„° ì…‹ë“¤ì´ ì •ê·œë¶„í¬ë¥¼ ì´ë£° ê²ƒì´ë¼ê³  ê°€ì •í•˜ê³ , ë°ì´í„°ì˜ ë¼ë²¨ì´ ì£¼ì–´ì ¸ ìˆì§€ ì•Šì„ ë•Œ, likelihood ë¹„êµë¡œ ë¼ë²¨ë§ í›„, ê° ê·¸ë£¹ë³„ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•œ ë’¤, ì¶”ì •ëœ ëª¨ìˆ˜ê°€ MLEì¸ì§€ ì•„ë‹Œì§€ í™•ì¸í•˜ê³ , íŒŒë¼ë¯¸í„° ì¶”ì •ê°’ì„ ë„ì¶œí•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.

ì—¬ê¸°ì„œ, Mixed Modelì€ ì „ì²´ ë¶„í¬ì—ì„œ í•˜ìœ„ ë¶„í¬ê°€ ì¡´ì¬í•œë‹¤ê³  ë³´ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.

ì¦‰, í•œ ê°œì˜ ë¶„í¬ê°€ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ê°œì˜ ë¶„í¬ë¡œë¶€í„° ìƒì„±ë˜ì—ˆë‹¤ê³  ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤. (Gaussian Mixture Modelì€ ì—¬ëŸ¬ ê°œì˜ ë¶„í¬ê°€ ì •ê·œë¶„í¬ë¡œ ë¶€í„° ìƒì„±ë˜ì—ˆë‹¤ê³  ë´…ë‹ˆë‹¤.

> **Gaussian Mixture Model(GMM : ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸)ì€ ìƒ˜í”Œì´ íŒŒë¼ë¯¸í„°ê°€ ì•Œë ¤ì§€ì§€ ì•Šì€ ì—¬ëŸ¬ ê°œì˜ í˜¼í•©ëœ ì •ê·œë¶„í¬ì—ì„œ ìƒì„±ë˜ì—ˆë‹¤ê³  ê°€ì •í•˜ëŠ” í™•ë¥  ëª¨ë¸ì…ë‹ˆë‹¤.**

> **Gaussian Mixture Model(GMM)ì€ generative model ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤. ìƒì„± ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ ë”°ë¥´ëŠ” ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.**

### Gaussian Mixture Model(GMM) ì‹¤ìŠµ

Scikit-Learnì˜ make_moons ë°ì´í„°ë¥¼ í†µí•´ GaussianMixture ì‹¤ìŠµì„ ì§„í–‰í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. make_moons í•¨ìˆ˜ëŠ” ì´ˆìŠ¹ë‹¬ ëª¨ì–‘ í´ëŸ¬ìŠ¤í„° ë‘ ê°œ í˜•ìƒì˜ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. make_moons ëª…ë ¹ìœ¼ë¡œ ë§Œë“  ë°ì´í„°ëŠ” ì§ì„ ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

![](https://images.velog.io/images/yunyoseob/post/27d31d37-d8b6-4bec-9c32-ca22d280bbad/image.png)

> **Scikit-Learnì˜ GaussainMixture í´ë˜ìŠ¤ë¡œ ê¸°ëŒ“ê°’-ìµœëŒ€í™” ì•Œê³ ë¦¬ì¦˜(EM ì•Œê³ ë¦¬ì¦˜)ì´ ì¶”ì •í•œ íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸ í•©ë‹ˆë‹¤.**

- **[Scikit-Learnì˜  GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)**

![](https://images.velog.io/images/yunyoseob/post/7d728f8e-eea1-4430-b78d-c499b4e1e22a/image.png)

> **GMMì—ì„œ score_samples() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬, í•´ë‹¹ ìœ„ì¹˜ì˜ í™•ë¥  ë°€ë„ í•¨ìˆ˜(PDF)ì˜ ë¡œê·¸ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì ìˆ˜ê°€ ë†’ì„ ìˆ˜ë¡ ë°€ë„ê°€ ë†’ìŠµë‹ˆë‹¤.**

```
input: print(f"gm.score_samples(X) \n {gm.score_samples(X)}")

output: gm.score_samples(X) 
 [-0.82889604 -1.07601095 -0.5781479  -1.62691711 -1.99954932 -1.68610971
 -0.71876098 -1.9475925  -1.09975276 -1.93872469 -1.80938555 -0.86157821
 -1.22472379 -0.56635783 -1.45547825 -2.18586591 -2.14784367 -0.41848486...
 
 ì´í•˜ ìƒëµ
```


> **Gaussian Mixture Modelì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ë„ íƒì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

**Confusion Matrix(í˜¼ë™ í–‰ë ¬)**

![](https://images.velog.io/images/yunyoseob/post/14b91650-4ea6-4af3-826c-ff84553f205a/image.png)

ë‹¤ìŒê³¼ ê°™ì´ Confusion Matrix(í˜¼ë™ í–‰ë ¬)ì´ ìˆë‹¤ê³  í•  ë•Œ, 

![](https://images.velog.io/images/yunyoseob/post/8c4d83ff-f76e-4d31-9721-a88bf3a0fe58/image.png)

ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì€ ì„œë¡œ íŠ¸ë ˆì´ë“œ ì˜¤í”„ ê´€ê³„ì…ë‹ˆë‹¤. 

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, thresholdë¥¼ ìˆ˜ì •í•˜ì—¬, imbalance targetì„ íƒì§€í•˜ëŠ”ë°, ì´ ë•Œë„ Gaussian Mixtureë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
densities=gm.score_samples(X)
density_thresold=np.percentile(densities, 4)
anomalies=X[densities<density_thresold]

plt.figure(figsize=(8, 4))

plot_gaussian_mixture(gm, X)
plt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*')
plt.ylim(top=5.1)

print("mixture_anomaly_detection_plot")
plt.show()
```

![](https://images.velog.io/images/yunyoseob/post/73d9b668-d597-46f3-a24f-6591f37a545b/image.png)

- ì´ìƒì¹˜ë¥¼ ë¹¨ê°„ ì ìœ¼ë¡œ í‘œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤.

> **Gaussian Mixture Model í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ì„ íƒ**

BIC(Bayesian Information Criterion)ì´ë‚˜ AIC(Akaike Information Critertion)ì„ ìµœì†Œí™” í•˜ëŠ” ëª¨ë¸ì„ ì°¾ìŠµë‹ˆë‹¤. (AICì™€ BICëŠ” í–¥í›„ ë³€ìˆ˜ì„ íƒë²• í¬ìŠ¤íŒ…ì—ì„œ ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. AICì™€ BICëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ì´ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.)

![](https://images.velog.io/images/yunyoseob/post/722fbf79-d3c3-4624-bd9a-9550cbea952e/image.png)

ë¹„ì§€ë„í•™ìŠµì—ì„œ K-means í´ëŸ¬ìŠ¤í„°ë§ì—ì„œ Kë¥¼ ê²°ì •í•  ë•Œ, ì‚¬ìš©í•˜ëŠ” Elbow Methodì²˜ëŸ¼
ê°€ìš°ì‹œì•ˆ í˜¼í•© ë¶„í¬ ëª¨ë¸ì—ì„œëŠ” BICì™€ AICê°€ ê°€ì¥ ì‘ì•„ ì§ˆë•Œì˜ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

> **Gaussian Mixture Model ì¤‘ì— ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì°¾ì§€ ì•ŠëŠ” BayesianGaussianMixture í´ë˜ìŠ¤ë„ ìˆìœ¼ë‚˜, ì´ëŠ” ë‹¤ìŒì— Bayesianì— ëŒ€í•œ ë‚´ìš©ì„ í¬ìŠ¤íŒ…í•  ë•Œ ë‹¤ë£¨ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.**

> **GMM ì‹¤ìŠµì€ [í•¸ì¦ˆì˜¨ ë¨¸ì‹ ëŸ¬ë‹ ì±…](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791162242964)ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì˜€ìœ¼ë©°, ì‹¤ìŠµì— ëŒ€í•œ ë‚´ìš©ì„ ë” ìì„¸íˆ ë³´ê³  ì‹¶ìœ¼ì‹œë©´, [handson-ml2/09_unsupervised_learning.ipynb](https://github.com/rickiepark/handson-ml2/blob/master/09_unsupervised_learning.ipynb)ë¥¼ ì°¸ê³ í•´ë³´ì…”ë„ ì¢‹ì„ ë“¯ í•©ë‹ˆë‹¤.ğŸ‘**

**ë§ˆì¹˜ë©°**
**ì˜¤ëŠ˜ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì •ê·œë¶„í¬ì— ëŒ€í•œ ì„¤ëª…ë¶€í„° ë°ì´í„°ë¶„ì„ì—ì„œ ì •ê·œë¶„í¬ê°€ í™œìš©ë˜ëŠ” ì‚¬ë¡€ê¹Œì§€ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤. íŠ¹íˆ, ë§ˆì§€ë§‰ì—” Gaussian Mixed Modelì— ëŒ€í•œ ì„¤ëª…ë¶€í„° ì‹¤ìŠµê¹Œì§€ êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ë£¨ì–´ë³´ì•˜ìŠµë‹ˆë‹¤. ì“°ë‹¤ë³´ë‹ˆê¹Œ GMM íŠ¹ì§‘ì´ ë˜ì–´ë²„ë¦° ê²ƒ ê°™ì§€ë§Œ, ê²°êµ­ í•µì‹¬ì€ ë°ì´í„° ë¶„ì„ì—ì„œ ë¹¼ë¨¹ì„ ìˆ˜ ì—†ëŠ” í‚¤ì›Œë“œì¤‘ í•˜ë‚˜ê°€ "ì •ê·œë¶„í¬(Gaussian Distribution)"ë¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ìƒìœ¼ë¡œ í¬ìŠ¤íŒ… ë§ˆì¹˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.ğŸ˜Š"**
